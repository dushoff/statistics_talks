EXTRA Todo

Inference loop


----------------------------------------------------------------------

EXTRA Uncurated

Might be a good idea to link back to the simulation-based validation that they’ve been doing in the labs at some point in this lecture; you came to SBV after I wrote this but still think linking it to what they’ve been doing would be pedagogically helpful
A bit harsh (but hilarious) to claim Farr is known for getting COVID wrong
The Ebola trial design example axes/etc are hard to read
Some references would be useful for people who want to learn more about actually doing these things
For all figures you’re showing from published studies, provide enough info on the slide that people could find the paper (eg a DOI)
Session is done at 9:17, scheduled for 9:30; suggests to me that you should be interacting with the participants more throughout the session

Abbott:

Love the Ptolemy v. Copernicus example. Maybe let them shock themselves vs. telegraphing so much (i.e., start with it and then use it to motivate what you are saying). As they all jumped the wrong way it might be that it would help them pay attention.
Getting a bit binary between models for understanding and prediction. 
Might be nice to use words like prior prediction, simulation based calibration etc. in case people are reading the bayesian literature.
Reference for the calibration study you are talking about.
Have you defined bias (even loosely?).
Maybe stress it can be an iterative process between simulation-based validation and model development when wrapping up.
Say what some goodness of fit tests are and what they are trying to do. It took me a minute to realise what you were talking about. Do people still use these a lot as their metric for deciding if their model is good/bad.
Ebola forecasting challenge is a nice example of a complex model being used for simulation and then simple models being used for fitting.
WOW a 3d plot.
I was expecting some more discussion about how to compare models in this lecture.
We didn’t talk about cross-validation (and measures used to approximate it) and what role it plays in our models (where we nearly always have a time component). We kind of touched on it when talking about out of sample validation but not enough to make the link in peoples minds if they have heard of it in other contexts (for example statistical modelling, ML etc.). 

----------------------------------------------------------------------


EXTRA History

Started at the second DAIDD, by request

Now given (optionally?) at MMED as well

Working on distinguishing; we already have DAIDD-only slides

Need to work on:

	DAIDD vs. MMED ×

	Note suppression -- um, what??
R


	Bias clarification

----------------------------------------------------------------------

NOFRAME

\newcommand{\jdtitle}{There are two titles}
\renewcommand{\jdtitle}{Model evaluation and comparison}
\renewcommand{\jdtitle}{Model assessment}
\newcommand{\jdsub}{\mbox{}}
\newcommand{\years}{2015--\thisyear} 

----------------------------------------------------------------------

ICI3DTHEME

----------------------------------------------------------------------

Roadmap

WIDEFIG my_images/road_map24a.pdf

----------------------------------------------------------------------

DAIDD Goals

	Discuss model types and model goals

	Explain the value of simulation for validating models

	Discuss metrics for evaluating fit

		Put the Goodness of fit test in its place

		Take a long digression about statistical philosophy

----------------------------------------------------------------------

MMED Goals

	Discuss model types and model goals

	Discuss the value of simulation for validating models

	Discuss metrics for evaluating fit

		Put the Goodness of fit test in its place

----------------------------------------------------------------------

Do I have a good model?

	What is my model trying to accomplish?

		Generating hypotheses

		Evaluating plausibility

		Prediction

		Mechanistic understanding

		Evaluating scenarios (guiding choices)

	Many models try to do more than one of these things

----------------------------------------------------------------------

DAIDD Statistical philosophy

PIC FIG webpix/obey_kitties.png

NOTES You should develop your own statistical philosophy

----------------------------------------------------------------------

SEC Conceptual models

----------------------------------------------------------------------

Disease thresholds

PIC FIG Endemic_curves/propCurves.Rout-0.pdf

----------------------------------------------------------------------

Effects of clinical immunity

FIG webpix/keeganClinicalDiagram.jpg

----------------------------------------------------------------------

Bistability

WIDEFIG webpix/keeganPhaseDiagram.jpg

----------------------------------------------------------------------

SEC Prediction

----------------------------------------------------------------------

Ptolemy v.\ Copernicus

PIC FIG webpix/epicycles.jpg

----------------------------------------------------------------------

PSLIDE Ptolemy v.\ Copernicus

PIC FIG webpix/heliocentric.jpg

----------------------------------------------------------------------

PSLIDE Ptolemy v.\ Copernicus

BCC

WFIG 0.94 webpix/epicycles.jpg

NCC

WFIG 0.89 webpix/heliocentric.jpg

EC

----------------------------------------------------------------------

Where will we see cholera cases?

PIC WIDEFIG my_images/Farr.png

----------------------------------------------------------------------

PSLIDE Where will we see cholera cases?

FIG webpix/choleraMap.jpg

----------------------------------------------------------------------

PSLIDE Where will we see cholera cases?

BCC

CFIG my_images/Farr.png

NCC

CFIG webpix/choleraMap.jpg

EC

----------------------------------------------------------------------

TSEC Model Validation

	Does your fitting algorithm match your \emph{model world}?

PIC HFIG 0.4 my_images/modelWorld.png

	If you use your fitting algorithm on simulations from your model
	world, then you \emph{know the right answer}!

	Simulation-based validation

		You should validate your model in the model world before doing anything
		else

----------------------------------------------------------------------

Validation measures

	Coverage!

	Precision

	Bias

	Accuracy

----------------------------------------------------------------------

Estimators

	What is an estimator?

		ANS An \emph{approach} for estimating a parameter (usually with confidence intervals)

		ANS The thing that you find using your estimator is an estimate

----------------------------------------------------------------------

Coverage

PRESENT HFIG 0.5 webpix/SierraLeoneValidation.jpg

NOTES HFIG 0.3 webpix/SierraLeoneValidation.jpg

	How often does your confidence interval \emph{cover} (include) the right
	answer (when you know it)?

	Your 95% confidence interval should have 95% coverage

		If more, your model is \emph{too conservative}

		If less, your model is \emph{invalid}

	In most cases it's good to look at the two tails separately:

		How often do you overestimate? Underestimate?

----------------------------------------------------------------------

Precision

	A good model tries to provide a precise answer 

		Precision means narrow confidence intervals

		\ldots But not at the price of overconfidence (invalidity)

	As data increases, your precision should increase

		CIs should approach zero width

		\ldots as long as you have data about \emph{everything}

	Conversely, CIs should reflect a variety of sources of uncertainty

----------------------------------------------------------------------

EXTRA Bias?

	Nobody wants to be biased 

		You shouldn't ignore sources of bias (confounders)

		Estimators with good coverage and good precision are
		\emph{asymptotically} unbiased

	Not so clear you need to be \emph{absolutely} unbiased

		Bias is the difference between the \emph{mean} expected prediction and the
		true value

	Scale dependence
	
		an unbiased estimate of $\gamma$ is automatically a biased estimate of
		$D$ (but not asymptotically biased)

		Medians (instead of means) can get avoid scale problems

----------------------------------------------------------------------

EXTRA Accuracy?

	Nobody wants to be inaccurate

	Good coverage and good precision should guarantee good accuracy

----------------------------------------------------------------------

Bias and accuracy

	Good coverage and high precision should ensure high accuracy and low bias

	Bias is the difference between the \emph{expected mean} of your estimator
	and the truth

		Your estimator doesn't need to be absolutely unbiased

		Your reasonable estimator will be asymptotically unbiased

	Accuracy is a measure of the \emph{expected error} of your estimator

----------------------------------------------------------------------

TSEC Model Evaluation

BC

PIC CFIG webpix/Earth.jpg

NC

	Does your model match the \emph{real world}?

		ANS No!

	What should we ask instead?

		ANS How close is it?

		ANS Is it good enough to answer a particular question?

EC

----------------------------------------------------------------------

TSS Goodness of fit

	Goodness of fit \emph{statistics} describe how well a model prediction
	matches observed data

	Goodness of fit \emph{tests} attempt to determine whether the observed
	difference between model and data is statistically significant

	Why don't I like goodness of fit tests?

----------------------------------------------------------------------

EXTRA A disease-incidence model

COMMENT Do I like this? Am I thinking clearly about it?

I like the picture at least.

SUBH Good for almost any disease

BC

HFIG 0.75 webpix/godDice.jpg

NC

	The gods roll dice to pick a probability between 0.1% and 10%.

	Each person on the planet gets the disease the next year with this
	probability

	$P>0.05$. My model is correct!

EC

----------------------------------------------------------------------

Your model is wrong!

SUBH \ldots or at least, incomplete

	A goodness of fit test won't make it true

	You can ``pass'' a goodness of fit test by:

		having a good model

		making very broad predictions

		having bad data

		choosing an inappropriate way to compare

	So why would we do this?

	For that matter, why do we use P values at all in biology?

----------------------------------------------------------------------

SS Digression

----------------------------------------------------------------------

OFFSLIDE Passing goodness of fit tests

BC

	I can make any model pass a goodness of fit test by broadening the
	uncertainty

	That doesn't make it a good model

NC

PIC CFIG webpix/godDice.jpg

EC

----------------------------------------------------------------------

DAIDD Vitamin A example

	We want to know if vitamin A supplements improve the health of
	village children

		Outcome: height growth in 6 months

	What does it mean if I find a "significant P value" for some effect
	in this experiment?

		ANS The difference is unlikely to be due to chance

		So what!  I already know vitamin A has strong effects on
		metabolism

	If I'm certain that the true answer isn't exactly zero, why do I
	want the P value anyway?

----------------------------------------------------------------------

DAIDD Vitamin study

FIG vitamins.Rout-0.pdf

----------------------------------------------------------------------


DAIDD Vitamin study

FIG vitamins.Rout-1.pdf

----------------------------------------------------------------------

DAIDD Discussion

	Do you agree that in biology we should assume that the answer to our
	sensible question is not exactly zero?

		Or at least have a philosophy consistent with that assumption?

			Can we ever \emph{prove} that an effect is zero?

	If we make that assumption (null hypothesis is false), why might we want a P value anyway?

----------------------------------------------------------------------

DAIDDREP Vitamin study

FIG vitamins.Rout-1.pdf

----------------------------------------------------------------------

DAIDD Annualized flu deaths

BC

CFIG flu.Rout-0.pdf

NC

	Why is weather not causing deaths at this time scale?

EC

----------------------------------------------------------------------

DAIDDREP ... with confidence intervals

BC

CFIG flu.Rout-1.pdf

NC

	\textbf{Never} say: A is significant and B isn't, so $A>B$

	\textbf{Instead:} Construct a statistic for the hypothesis $A>B$

		May be difficult

EC

----------------------------------------------------------------------

MMEDREP Low P values

FIG webpix/clear.jpg

----------------------------------------------------------------------

MMEDREP High P values

FIG webpix/fog.jpg

----------------------------------------------------------------------

MMED What does the P value mean?

	Low: you are seeing something clearly

	High: you are seeing something unclearly

----------------------------------------------------------------------

DAIDD Low P values

BC

	If I have a low P value I can see something clearly

	But it's usually better to focus on what I see than the P value

NC

SIDEFIG webpix/clear.jpg

EC


----------------------------------------------------------------------

DAIDD High P values

BC

	If I have a high P value, there is something I \emph{don't} see
	clearly

	It \emph{may be} because this effect is small

	High P values should \emph{not} be used to advance your conclusion

NC

SIDEFIG webpix/fog.jpg

EC

----------------------------------------------------------------------


Goodness of fit test

	Your model is \emph{not} reality (null hypothesis is false)

	Can we see the difference clearly?

		If \emph{no}, model may be \emph{good} or \emph{bad}. 

			We probably can't add more complexity based on current
			data

			If we need to know more, we need more data

		If \emph{yes}, model may be \emph{good} or \emph{bad}. 

			We \emph{may} be able to add more complexity based on current data

			But we may not need to

	RESTING Reward and punishment (what did I mean by this?)

----------------------------------------------------------------------

Capturing patterns

	You can ask:

		Does your model do a reasonable job of capturing the data?

			You can use a goodness of fit \emph{statistic} for this, and not
			worry about the P value

		Does your model capture patterns and relationships that you (or
		other experts) think are important?

----------------------------------------------------------------------

SS Going beyond

----------------------------------------------------------------------

Out-of-sample validation

	Does your model make predictions \emph{outside} the range on which you
	calibrated it? (that is, found the parameters)

		Predicting gravitational shifts in star positions from
		measurements in Earth laboratories

		Matching cholera outbreaks in Bangladesh with a model calibrated
		to Haiti

		Predicting influenza patterns in 2010 from a model calibrated from
		2000--2009

----------------------------------------------------------------------

Predicting way out of sample

PIC HFIG 0.7 webpix/prometheus.jpg

NOTES Saturn's shepherd moons were predicted before they were seen!

Essentially, all models are wrong, but some are useful.

-- Box and Draper (1987), \emph{Empirical Model Building \ldots}

----------------------------------------------------------------------

Test sets

	What is \textbf{test set} spelled backwards?

	Hold some data out while fitting your model

	Or just \emph{pretend} to do this as an evaluation method

		In other words, test what would happen under various withholding
		scenarios

		Cross-validation!

----------------------------------------------------------------------

Other model worlds

	The model you're \emph{fitting} is probably pretty simple

	But you can \emph{simulate} very complicated models, indeed

PIC HFIG 0.6 webpix/legoland.jpg

	How well can you do? Which details are important?

----------------------------------------------------------------------

PSLIDE Other model worlds

FIG my_images/T34.NIH.compare-0.pdf

----------------------------------------------------------------------

PSLIDE Other model worlds

FIG my_images/T34.NIH.compare-1.pdf

----------------------------------------------------------------------

PSLIDE Generating hypotheses

FIG webpix/EbolaBurial.jpg

----------------------------------------------------------------------

PSLIDE Generating hypotheses

FIG webpix/lembo_spp.jpg

----------------------------------------------------------------------

NSLIDE Generating hypotheses

For example:

	Safe burial is key to interrupting Ebola transmission

	Vaccinating domestic dogs can eliminate transmission of canine rabies

----------------------------------------------------------------------

PSLIDE Testing hypotheses

WIDEFIG my_images/Farr.png

----------------------------------------------------------------------

PSLIDE Testing hypotheses

FIG webpix/choleraMap.jpg

----------------------------------------------------------------------

PSLIDE Testing hypotheses

FIG webpix/BroadStreet.jpg

----------------------------------------------------------------------

NSLIDE Testing hypotheses

	Both the Farr model and the Snow model made testable predictions about
	cholera

	Snow tested his hypotheses by removing the pump handle

----------------------------------------------------------------------

Hard questions

PIC FIG webpix/QuestionAnswers.jpg

Answers are not always easy

----------------------------------------------------------------------

TSS Inference

	Do you have a specific question?

		Should we close schools?

		Do we need to investigate sexual transmission?

		Are sub-clinical infections important?

		Should we prepare for a disease surge?

----------------------------------------------------------------------

Validate the inference

	Do different parameters (or different models) give different answers to
	important questions?

		Or show importantly different patterns?

	What do I need to know (either now or in the future) to clarify these
	answers?

		That is, to make my model conclusions \emph{robust}?

	Use model insights to guide data collection

----------------------------------------------------------------------

SEC Conclusion

----------------------------------------------------------------------

Summary

SUBH Dynamic models

	Clarify thinking

		What are our assumptions, what else do we need to know?

	Understand outcomes

		Can heterogeneity explain the time course of HIV epidemics?

		Is it possible that MDA could break the cycle of malaria transmission in
		some areas?

	Predict outcomes

		What is the potential for a hepatitis A outbreak in Cape Town?

		What might happen if I improve testing-and-treatment outreach in Jamaica?

	Find new mechanisms

		Why can't I explain my data? What haven't I thought of?

----------------------------------------------------------------------

Summary

SUBH Evaluation

	Validation (inside your model world)

		Does my fitting method work (assuming my model is right)?

	Inspection (compare patterns)

	Prediction (and other out-of-sample comparison)

		Can my model predict things I haven't told it yet?

	Inference

		Does my model give me a good idea what to do next?

		Does my model's \emph{failure} give me a good idea what to \emph{learn}
		next?

	Generate and test mechanistic hypotheses
