
EXTRA FRONT

There is no such thing as statistical "significance": bridging gaps between applied science and theoretical statistics

I will discuss why I think statistical philosophy is important for quantitative scientists, and some lessons I have learned while transitioning as a scientist from a mathematical orientation to a more statistical orientation scientists. I will argue that people who apply statistics to scientific problems need a different view of error than theoreticians who develop statistical methods, and that statistical "significance" is not a thing.

----------------------------------------------------------------------

EXTRA To-do

* plainset

* front material

* ideas (material from evaluation, article titles and text)

----------------------------------------------------------------------

NOFRAME

\newcommand{\jdtitle}
{Is Statistical ``Significance" a Thing?}
\newcommand{\jdsub}
{Bridging between science and statistical theory}
\newcommand{\jdauth}{Jonathan Dushoff, McMaster University}
\newcommand{\years}{2012--2017} 

----------------------------------------------------------------------

ICI3D

----------------------------------------------------------------------

PSLIDE NOFRAME

{\topBar\maketitle}

----------------------------------------------------------------------

NSLIDE NOFRAME

{\let\newpage\relax\maketitle}

----------------------------------------------------------------------

Topics

	How statistics fits into science (statistical philosophy)

	The role of P values (and the cult of P values)

	Theoretical vs applied perspectives on error

	What's significant about significance?

----------------------------------------------------------------------

TSEC Statistical inference

	Scientists use statistics to confirm effects, estimate parameters, and
	predict outcomes

	It usually rains when I'm in Cape Town, but mostly on Sunday

		\emph{Confirmation:} In Cape Town, it rains more on Sundays than
		other days

		\emph{Estimation:} In Cape Town, the \emph{odds} of rain on
		Sunday are 1.6--2.2 times higher than on other days

		\emph{Prediction:} I am confident that it will rain at least one
		Sunday the next time I go

----------------------------------------------------------------------

Raining in Cape Town

BC

	How we interpret data like this necessarily depends on assumptions:

		Is it likely our observations occured by chance?

		Is it likely they \emph{didn't}?

NC

PIC HFIG 0.8  my_images/eight.jpg

PIC CREDIT Tessa Wessels, {\em Faces on a Train}

EC

----------------------------------------------------------------------

Vitamin A

	We compare health indicators of children treated or not treated with
	vitamin A supplements 

		\emph{Estimate:} how much taller (or shorter) are the treated
		children on average?

		\emph{Confirmation:} are we sure that the supplements are helping
		(or hurting)?

		\emph{Range of estimates:} how much do we think the supplement is
		helping?

----------------------------------------------------------------------

TSS P values and confidence intervals

	We use \emph{P values} to say how sure we are that we have seen a positive 
	effect

	We use \emph{confidence intervals} to say what we think is going on
	(with a certain level of confidence)

	P values are \emph{over-rated}

	\emph{Never} use a high P value as evidence for anything, e.g.:
	
		that an effect is small

		that two quantities are similar

----------------------------------------------------------------------

Vitamin A questions?

	Is vitamin A good for these children?

	How sure are we?

	How good do we think it is?

	How sure are we about that?

----------------------------------------------------------------------

P values

	What does it mean if I find a ``significant P value" for some effect
	in this experiment?

	The difference is unlikely to be due to chance

		So what!  I already know vitamin A has strong effects on
		metabolism

	If I'm certain that the true answer isn't exactly zero, why do I
	want the P value anyway?

----------------------------------------------------------------------

Confidence intervals

BC

CFIG vitamins.Rout-0.pdf

NC

	What do these results mean?

	Which are significant?

EC

----------------------------------------------------------------------

Confidence intervals and P values

BC

PRESENT CFIG vitamins.Rout-1.pdf

NC

	A high P value means we can't see the sign of the effect clearly

	A low P value means we can

EC

----------------------------------------------------------------------

The meaning of P values

PIC DBFIG 0.8 webpix/fog.jpg 1 webpix/clear.jpg

	More broadly, a P value measures whether we are seeing
	\emph{something} clearly

		It's usually the sign ($\pm$) of some quantity, but doesn't need to be

----------------------------------------------------------------------

Types of Error

	Type I (\emph{False positive:}) concluding there is an effect when
	there isn't one

		This doesn't happen in biology.  There is always an effect.

	Type II (\emph{False negative:}) concluding there is no effect when
	there really is

		This \emph{should} never happen in biology, because we should never conclude
		there is no effect

		In fact, it happens all the time

----------------------------------------------------------------------

RHEAD Types of Error

	Type III Error is the error of using numerical codes for things that have perfectly good simple names

	Just say ``false positive'' or ``false negative'' when possible

----------------------------------------------------------------------

Errors in applied studies

BC

	\emph{Sign error:} if I think an effect is positive, when it's really
	negative (or vice versa)

	\emph{Magnitude error:} if I think an effect is small, when it's
	really large (or vice versa)

	Confidence intervals clarify all of this

NC

SIDEFIG webpix/error.jpg

EC

----------------------------------------------------------------------

Errors in theoretical stuides

	\emph{False positive:} in the hypothetical case that the
	effect is exactly zero, what is the probability of falsely finding
	an effect

		Should be less than or equal to my significance value

	\emph{False negative:} what is the probability of failing
	to find an effect that is there?

		Requires you specify a hypothetical effect \emph{size}

		This is a scientific judgment

	These are useful to analyze \textbf{power} and \textbf{validity} of a
	statistical design
	
		You should do these analyses \emph{before} you collect data, not after

----------------------------------------------------------------------

Low P values

BC

	If I have a low P value I can see something clearly

	But it's usually better to focus on what I see than the P value

NC

SIDEFIG webpix/clear.jpg

EC


----------------------------------------------------------------------

High P values

BC

	If I have a high P value, there is something I \emph{don't} see
	clearly

	It \emph{may be} because this effect is small

	High P values should \emph{not} be used to advance your conclusion

NC

SIDEFIG webpix/fog.jpg

EC

----------------------------------------------------------------------

What causes high P values?

	Small differences

	Less data

	More noise

	An inappropriate model

	Less model resolution

	A lower P value means that your evidence for difference is better

	A higher P value means that your evidence for similarity is
	better -- or worse!

----------------------------------------------------------------------

Annualized flu deaths

BC

CFIG flu.Rout-0.pdf

NC

	Why is weather not causing deaths at this time scale?

EC

----------------------------------------------------------------------

DEFHEAD ... with confidence intervals

BC

CFIG flu.Rout-1.pdf

NC

	\textbf{Never} say: A is significant and B isn't, so $A>B$

	\textbf{Instead:} Construct a statistic for the hypothesis $A>B$

		May be difficult

EC

----------------------------------------------------------------------

SS Statistics and science

----------------------------------------------------------------------

Syllogisms

BC

	All men are mortal

	Jacob Zuma is mortal

	Therefore, Jacob Zuma is a man

NC

SIDEFIG webpix/zuma.jpg

EC

----------------------------------------------------------------------

Syllogisms

BC

	All men are mortal

	Fanny the elephant is mortal

	Therefore, Fanny is a man

NC

SIDEFIG webpix/fanny.jpg

EC

----------------------------------------------------------------------

Bad logic

	A lot of statistical practice works this way:

		bad logic in service of conclusions that are (usually) correct

	This sort of statistical practice leads in the aggregate to bad
	science

	The logic can be fixed:

		Estimate a difference, or an interaction

----------------------------------------------------------------------

Small effects

	We can't build statistical confidence that something is small by
	failing to see it clearly

	We must instead see clearly that it is small

	This means we need a standard for what we mean by small

----------------------------------------------------------------------

PSLIDE Flu masks 

DBFIG 0.8 webpix/N95.jpg 0.7 webpix/surgical.jpg

----------------------------------------------------------------------

Flu mask example

	People who work in respiratory clinics sometimes have to wear bulky,
	uncomfortable, expensive masks

	They would like to switch to simpler masks, if those will do the job

	How can this be tested statistically?  We don't want the masks to be
	``different".

		Use a confidence interval

		Decide how big a level is acceptable, and construct a P value for
		the hypothesis that this level is excluded!

----------------------------------------------------------------------

Study results

FIG masks.Rout-2.pdf

----------------------------------------------------------------------

RHEAD Non-inferiority trial

BC

PRESENT CFIG masks.Rout-0.pdf

NC

	Is the new mask ``good enough"?

	What's our standard for that?

EC

----------------------------------------------------------------------

Non-inferiority trial

BC

CFIG masks.Rout-1.pdf

NC

	We can even attach a P value by basing it on the ``right" statistic.

	The right statistic is the thing whose sign we want to know:

		The difference between the observed effect and the standard we
		chose

EC

----------------------------------------------------------------------


TSEC Model Evaluation

BC

CFIG webpix/Earth.jpg

NC

	Does your model match the \emph{real world}?

		ANS No!

	How well does your model match the real world?

EC

----------------------------------------------------------------------

TSS Goodness of fit

	Goodness of fit \emph{statistics} describe how well a model prediction
	matches observed data

	Goodness of fit \emph{tests} attempt to determine whether the observed
	difference between model and data is statistically significant

----------------------------------------------------------------------

EXTRA A disease-incidence model

COMMENT Do I like this? Am I thinking clearly about it?

I like the picture at least.

SUBH Good for almost any disease

BC

HFIG 0.75 webpix/godDice.jpg

NC

	The gods roll dice to pick a probability between 0.1% and 10%.

	Each person on the planet gets the disease the next year with this
	probability

	$P>0.05$. My model is correct!

EC

----------------------------------------------------------------------

Your model is false!

	A goodness of fit test won't make it true

	You can ``pass'' a goodness of fit test by:

		having a good model

		making very broad predictions

		having bad data

		choosing an inappropriate way to compare

	So why would we do this?

	For that matter, why do we use P values at all in biology?


----------------------------------------------------------------------

Passing goodness of fit tests

BC

CFIG webpix/godDice.jpg

NC

	I can make any model pass a goodness of fit test but broadening the uncertainty

	That doesn't make it a good model

EC

----------------------------------------------------------------------

DAIDD Vitamin A example

	We want to know if vitamin A supplements improve the health of
	village children

		Outcome: height growth in 6 months

	What does it mean if I find a ``significant P value" for some effect
	in this experiment?

		ANS The difference is unlikely to be due to chance

		So what!  I already know vitamin A has strong effects on
		metabolism

	If I'm certain that the true answer isn't exactly zero, why do I
	want the P value anyway?

----------------------------------------------------------------------

DAIDD Group activity

	Do you agree that in biology we should assume that the answer to our
	sensible question is not exactly zero?

		Or at least have a philosophy consistent with that assumption?

			Can we ever \emph{prove} that an effect is zero?

	If we make that assumption (null hypothesis is false), why might we want a P value anyway?

----------------------------------------------------------------------

MMEDREP Vitamin study

FIG vitamins.Rout-0.pdf

----------------------------------------------------------------------

PSLIDE Vitamin study

FIG vitamins.Rout-1.pdf

----------------------------------------------------------------------

DAIDD Annualized flu deaths

BC

CFIG flu.Rout-0.pdf

NC

	Why is weather not causing deaths at this time scale?

EC

----------------------------------------------------------------------

DAIDDREP ... with confidence intervals

BC

CFIG flu.Rout-1.pdf

NC

	\textbf{Never} say: A is significant and B isn't, so $A>B$

	\textbf{Instead:} Construct a statistic for the hypothesis $A>B$

		May be difficult

EC

----------------------------------------------------------------------

MMED PSLIDE Low P values

FIG webpix/clear.jpg

----------------------------------------------------------------------

MMED PSLIDE High P values

FIG webpix/fog.jpg

----------------------------------------------------------------------

MMED What does the P value mean?

	Low: you are seeing something clearly

	High: you are seeing something unclearly

----------------------------------------------------------------------

DAIDD Low P values

BC

	If I have a low P value I can see something clearly

	But it's usually better to focus on what I see than the P value

NC

SIDEFIG webpix/clear.jpg

EC


----------------------------------------------------------------------

DAIDD High P values

BC

	If I have a high P value, there is something I \emph{don't} see
	clearly

	It \emph{may be} because this effect is small

	High P values should \emph{not} be used to advance your conclusion

NC

SIDEFIG webpix/fog.jpg

EC

----------------------------------------------------------------------


Goodness of fit test

	Your model is \emph{not} reality (null hypothesis is false)

	Can we see the difference clearly?

		If no, model may be good or bad. 
		
			We probably can't add any more complexity based on current
			data

		If yes, model may be good or bad. We \emph{may} be able to add more
		complexity based on current data

			But we may not need to

----------------------------------------------------------------------

TSS Capturing patterns

	You can ask:

		Does your model do a reasonable job of capturing the data?

			You can use a goodness of fit \emph{statistic} for this, and not
			worry about the P value

		Does your model capture patterns and relationships that you (or
		other experts) think are important?

----------------------------------------------------------------------

SS Going beyond

----------------------------------------------------------------------

Out-of-sample validation

	Does your model make predictions \emph{outside} the range on which you
	calibrated it?

		Predicting gravitational shifts in star positions from
		measurements in Earth laboratories

		Predicting cholera outbreaks in Bangladesh from a model calibrated
		to Haiti

		Predicting influenza patterns in 2010 from a model calibrated from
		2000--2009

----------------------------------------------------------------------

Test sets

	What is \textbf{test set} spelled backwards?

	Hold some data out while fitting your model

	Or just \emph{pretend} to do this as an evaluation method

		In other words, test what would happen under various withholding
		scenarios

----------------------------------------------------------------------

Other model worlds

	The model you're \emph{fitting} is probably pretty simple

	But you can \emph{simulate} very complicated models, indeed

PIC HFIG 0.6 webpix/legoland.jpg

	How well can you do? Which details are important?

----------------------------------------------------------------------

PSLIDE Other model worlds

FIG my_images/T34.NIH.compare-0.pdf

----------------------------------------------------------------------

PSLIDE Other model worlds

FIG my_images/T34.NIH.compare-1.pdf

----------------------------------------------------------------------

PSLIDE Generating hypotheses

FIG webpix/EbolaBurial.jpg

----------------------------------------------------------------------

PSLIDE Generating hypotheses

FIG webpix/lembo_spp.jpg

----------------------------------------------------------------------

NSLIDE Generating hypotheses

For example:

	Safe burial is key to interrupting Ebola transmission

	Vaccinating domestic dogs can eliminate transmission of canine rabies

----------------------------------------------------------------------

PSLIDE Testing hypotheses

WIDEFIG my_images/Farr.png

----------------------------------------------------------------------

PSLIDE Testing hypotheses

FIG webpix/choleraMap.jpg

----------------------------------------------------------------------

PSLIDE Testing hypotheses

FIG webpix/BroadStreet.jpg

----------------------------------------------------------------------

NSLIDE Testing hypotheses

	Both the Farr model and the Snow model made testable predictions about
	cholera

	Snow tested his hypotheses by removing the pump handle

----------------------------------------------------------------------

Hard questions

PIC FIG webpix/QuestionAnswers.jpg

Answers are not always easy

----------------------------------------------------------------------

SEC Conclusion

----------------------------------------------------------------------

Summary

SUBH Dynamic models

	Clarify thinking

		What are our assumptions, what else do we need to know?

	Understand outcomes

		Can heterogeneity explain the time course of HIV epidemics?

		Is it possible that MDA could break the cycle of malaria transmission in
		some areas?

	Predict outcomes

		What is the potential for a hepatitis A outbreak in Cape Town?

		What might happen if I improve testing-and-treatment outreach in Jamaica?

	Find new mechanisms

		Why can't I explain my data? What haven't I thought of?

----------------------------------------------------------------------

Summary

SUBH Evaluation

	Validation (inside your model world)

		Does my fitting method work (assuming my model is right)?

	Inspection (compare patterns)

	Prediction (and other out-of-sample comparison)

		Can my model predict things I haven't told it yet?

	Generate and test mechanistic hypotheses

----------------------------------------------------------------------

Conclusion

PIC HFIG 0.7 webpix/prometheus.jpg

NOTES Saturn's shepherd moons were predicted before they were seen!

Essentially, all models are wrong, but some are useful.

-- Box and Draper (1987), \emph{Empirical Model Building \ldots}


----------------------------------------------------------------------

----------------------------------------------------------------------

Your philosophy

	Statistics are not a magic machine that gives you the right answer

	If you are to be a serious scientist in a noisy world, you should
	have your own philosophy of statistics

		Be pragmatic: your goal is to do science, not get caught by
		theoretical considerations

		Be honest: it's harder than it sounds.  

----------------------------------------------------------------------

Honesty

	You can always keep analyzing until you find a ``significant''
	result

		If you do this you will make a lot of mistakes

	You may also keep analyzing until you find a result that you already
	``know'' is true. 

		This is confirmation bias; you're probably right, but your project is not
		advancing science

	Good practice

		Keep a data-analysis journal

		Start \emph{before} you look at the data

----------------------------------------------------------------------

PSLIDE Summary

FIG webpix/pipe.jpg

----------------------------------------------------------------------

Summary

	P values are over-rated

	High P values should not be used as evidence for anything ever.

		They can provide indirect evidence. Wonderful. Find the direct evidence
		and use that instead.

	Use effect sizes and confidence intervals when you can

	Otherwise, find ways to make significant P values do the work
	
		Non-inferiority tests, interactions

----------------------------------------------------------------------

RHEAD Summary

	Frequentist statistics makes weak assumptions, and finds logically weak
	formal conclusions:

		These parameters are unlikely to produce a statistic this extreme by
		chance

	Bayesian statistics makes strong assumptions:
	
		prior distributions must be fully specified
		
	\ldots and finds logically strong 
	formal conclusions:

		The probability that the effect value is in this range is X

		These strong conclusions can be used directly for prediction with
		uncertainty

----------------------------------------------------------------------

RHEAD Summary

	Statistics are a key component of data-based science

		You should think about statistical analysis from the beginning of your
		project

	You need a basic understanding of statistical principles

	You need your own statistical philosophy

		If you're a scientist, it should be pragmatic and honest

----------------------------------------------------------------------

PSLIDE Philosophy

	If you're a scientist, it should be pragmatic and honest

	If you're a theoretician, it should be ideological and honest

	If you're a capitalist, it should be pragmatic and dishonest

	If you're a politician, it should be ideological and dishonest

----------------------------------------------------------------------

